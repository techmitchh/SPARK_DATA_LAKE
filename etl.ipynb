{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "trailing comma not allowed without surrounding parentheses (<ipython-input-209-3d6eff59d855>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-209-3d6eff59d855>\"\u001b[1;36m, line \u001b[1;32m9\u001b[0m\n\u001b[1;33m    from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, dayofweek, date_format,\u001b[0m\n\u001b[1;37m                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m trailing comma not allowed without surrounding parentheses\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import TimestampType, DateType\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, dayofweek, date_format,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import AWS Key and Secret into OS variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['IAM']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['IAM']['AWS_SECRET_ACCESS_KEY']\n",
    "key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "secret = os.getenv(\"AWS_SECRET_ACCESS_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Sparkify_Lake\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.fast.upload\",\"true\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", key)\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Songs and Logs Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_songs = \"s3a://udacity-dend/song_data/A/A/A/TRAAAAK128F9318786.json\"\n",
    "load_logs =  \"s3a://udacity-dend/log_data/2018/11/2018-11-01-events.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: string (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Spark reads the song data and prints it's schema\n",
    "song_data = spark.read.json(load_songs)\n",
    "song_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Spark reads the log data and prints it's schema\n",
    "log_data = spark.read.json(load_logs)\n",
    "log_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Song and Artist Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract columns to create songs table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+------------------+----+--------+\n",
      "|           song_id| title|         artist_id|year|duration|\n",
      "+------------------+------+------------------+----+--------+\n",
      "|SOBLFFE12AF72AA5BA|Scream|ARJNIUY12298900C91|2009|213.9424|\n",
      "+------------------+------+------------------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# song_id, title, artist_id, year, duration\n",
    "songs_table = song_data[['song_id', 'title', 'artist_id', 'year','duration']]\n",
    "songs_table.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract columns to create artists table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------+---------------+---------------+----------------+\n",
      "|         artist_id| artist_name|artist_location|artist_latitude|artist_longitude|\n",
      "+------------------+------------+---------------+---------------+----------------+\n",
      "|ARJNIUY12298900C91|Adelitas Way|               |           null|            null|\n",
      "+------------------+------------+---------------+---------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# artist_id, name, location, latitude, longitude\n",
    "artists_table = song_data[['artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude']]\n",
    "artists_table.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write songs table to parquet files partitioned by year and artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_table.write.mode(\"overwrite\").partitionBy(\"year\", \"artist_id\").parquet(\"songs.parquet\")\n",
    "songs_par = spark.read.parquet('songs.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------+--------+----+------------------+\n",
      "|           song_id| title|duration|year|         artist_id|\n",
      "+------------------+------+--------+----+------------------+\n",
      "|SOBLFFE12AF72AA5BA|Scream|213.9424|2009|ARJNIUY12298900C91|\n",
      "+------------------+------+--------+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songs_par.createOrReplaceTempView(\"songs_view\")\n",
    "s_view = spark.sql(\"SELECT * FROM songs_view\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write artists table to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists_table.write.mode(\"overwrite\").parquet(\"artists.parquet\")\n",
    "artists_par = spark.read.parquet('artists.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------+---------------+---------------+----------------+\n",
      "|         artist_id| artist_name|artist_location|artist_latitude|artist_longitude|\n",
      "+------------------+------------+---------------+---------------+----------------+\n",
      "|ARJNIUY12298900C91|Adelitas Way|               |           null|            null|\n",
      "+------------------+------------+---------------+---------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artists_par.createOrReplaceTempView(\"artists_view\")\n",
    "a_view = spark.sql(\"SELECT * FROM artists_view\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Log Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get timestamp from ts column in log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_timestamp = udf(lambda x: datetime.fromtimestamp(int(int(x) / 1000)), TimestampType())\n",
    "convert_time = log_data.select('*', get_timestamp(log_data.ts).alias('start_time'))\n",
    "convert_time.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary sql views\n",
    "convert_time.createOrReplaceTempView(\"convert_time_log\")\n",
    "song_data.createOrReplaceTempView(\"song_data_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine data from convert_time_log and  song_data_table to create Songplays table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songsplays = spark.sql('''\n",
    "                        SELECT ct.start_time,\n",
    "                               ct.userid as user_id,\n",
    "                               level,\n",
    "                               s.song_id,\n",
    "                               s.artist_id,\n",
    "                               ct.sessionId,\n",
    "                               ct.location,\n",
    "                               ct.userAgent as user_agent\n",
    "                          FROM convert_time_log as ct\n",
    "                          LEFT JOIN song_data_table as s\n",
    "                            ON ct.artist = s.artist_name\n",
    "                         WHERE ct.page = \"NextSong\"\n",
    "                      ''').show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract columns for users table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------+------+-----+\n",
      "|userId|firstName|lastName|gender|level|\n",
      "+------+---------+--------+------+-----+\n",
      "|    39|   Walter|    Frye|     M| free|\n",
      "|     8|   Kaylee| Summers|     F| free|\n",
      "|     8|   Kaylee| Summers|     F| free|\n",
      "+------+---------+--------+------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# user_id, first_name, last_name, gender, level\n",
    "users = log_data[['userId', 'firstName', 'lastName', 'gender', 'level']]\n",
    "users.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write users table to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------+------+-----+\n",
      "|userId|firstName|lastName|gender|level|\n",
      "+------+---------+--------+------+-----+\n",
      "|    39|   Walter|    Frye|     M| free|\n",
      "|     8|   Kaylee| Summers|     F| free|\n",
      "|     8|   Kaylee| Summers|     F| free|\n",
      "+------+---------+--------+------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users.write.mode(\"overwrite\").parquet(\"users.parquet\")\n",
    "users_par = spark.read.parquet('users.parquet')\n",
    "users_par.select('*').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Extract columns to create time table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+---+----+-----+----+-------+\n",
      "|start_time|hour|day|week|month|year|weekday|\n",
      "+----------+----+---+----+-----+----+-------+\n",
      "|  15:57:10|  15|  1|  44|   11|2018|      5|\n",
      "|  16:01:46|  16|  1|  44|   11|2018|      5|\n",
      "|  16:01:46|  16|  1|  44|   11|2018|      5|\n",
      "|  16:02:12|  16|  1|  44|   11|2018|      5|\n",
      "|  16:05:52|  16|  1|  44|   11|2018|      5|\n",
      "+----------+----+---+----+-----+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# year, month, dayofmonth, hour, weekofyear, date_format\n",
    "time_data = log_data.select('ts', get_timestamp(log_data.ts).alias('start_time'))\n",
    "                    \n",
    "time = time_data.select(\n",
    "                        date_format('start_time', 'HH:mm:ss').alias('start_time'),\\\n",
    "                        hour('start_time').alias('hour'),\\\n",
    "                        dayofmonth('start_time').alias('day'),\\\n",
    "                        weekofyear('start_time').alias('week'),\\\n",
    "                        month('start_time').alias('month'),\\\n",
    "                        year('start_time').alias('year'),\\\n",
    "                        dayofweek('start_time').alias('weekday')\n",
    "                        )\n",
    "time.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write users table to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+---+----+-----+----+-------+\n",
      "|start_time|hour|day|week|month|year|weekday|\n",
      "+----------+----+---+----+-----+----+-------+\n",
      "|  15:57:10|  15|  1|  44|   11|2018|      5|\n",
      "|  16:01:46|  16|  1|  44|   11|2018|      5|\n",
      "|  16:01:46|  16|  1|  44|   11|2018|      5|\n",
      "+----------+----+---+----+-----+----+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time.write.mode(\"overwrite\").parquet(\"time.parquet\")\n",
    "time_par = spark.read.parquet('time.parquet')\n",
    "time_par.select('*').show(3)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
